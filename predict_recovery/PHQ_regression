##PHQ 9 analysis
library(agricolae)
library(lmerTest)
library(data.table)
library(corrplot)
library(glmnet)
library(caret)
library(groupdata2)
library(readxl)
library(multcomp)
library(tidyverse)
library(lubridate)
library(naniar)
library(missMethods)
library(openxlsx)
library(psych)
library(pROC)

select <- dplyr::select

#load datasets 
getwd()
# setwd("C:/Users/k1754359/OneDrive - King's College London/PhD/6. Correlation Digital signals in Depression/R scripts")
# download <- "write.csv(, C:/Users/k1754359/Downloads/.csv)"
# setwd("C:/Users/valer/Downloads")

redcap_full <- read_excel("REDcap full.xlsx")
IDmap <- read_excel("4. aRMT data.xlsx", sheet = "IDmap") |>
  rename(p_id = participant_id)


options(scipen=999)
# options(scipen=0) # to revert scientific notation


sleep <- fread("daily_sleep_feature.csv", data.table=F) %>%
  dplyr::mutate(date_str = lubridate::ymd(date_str))

bt <- fread("daily_bt_feature.csv", data.table=F) %>%
  dplyr::mutate(date_str = lubridate::ymd(date_str))

gps <- fread("daily_GPS_feature.csv", data.table=F) %>%
  dplyr::mutate(date_str = lubridate::ymd(date_str))



## Ends with phq_select <- outcome

# PHQ9 cleaning-----
#create a function to fix the incorrect scoring of scales.
rescore <- function(x, na.rm = FALSE) (x - 1)
phq_select <- redcap_full %>%
  dplyr::select(c(1, 2, contains("phq9"))) %>%
  filter(record_id > 13) %>%
  filter(str_detect(redcap_event_name, "week|enrolment")) %>% 
  #PHQ was coded incorrectly from 1 - 4 instead of 0 -3, so need to fix  
  mutate(across(phq9_1:phq9_10, rescore)) %>%
  mutate(total_phq = rowSums(across(phq9_1:phq9_9), na.rm = T)) %>%
  filter(!is.na(phq9_timestamp)) %>%    #select only one row per enrolment event
  # dichotomise phq9 based on clinical cut-off
  rename(redcap_event = redcap_event_name) %>% 
  mutate(phq_binary = ifelse(total_phq >9, 1, 0))
#change redcap_event with week number
redcap_event <- phq_select$redcap_event  %>%
  str_remove_all("_arm_1") %>%
  str_remove_all("week_") %>%
  str_replace("enrolment", "0")
phq_select$redcap_event <- as.numeric(redcap_event)
phq_select$survey_date <- as.Date(phq_select$phq9_timestamp)

#add p_id
phq <- merge(phq_select, IDmap[, c("p_id", "record_id")])

# ADD features to PHQ (following Yuezhou's code from QIDS)
qids<-phq




win_size <- 7

# Sleep -----

# extract second-order features
sleep_feature <- colnames(sleep)[6:15]
second_feature <- c("_mean", "_std")
new_feature =c()
for (i in 1:length(sleep_feature)){
  for (j in 1:length(second_feature)){
    new_feature = append(new_feature,paste(sleep_feature[i],second_feature[j],sep = ""))
  }
}
pre_col_num <- ncol(qids)
qids$sleep_day <- NA
qids[, new_feature] <-NA

for (i in 1:nrow(qids)){
  print(i)
  p_id_select <- qids$p_id[i]
  date_select <- qids$survey_date[i]
  sleep_temp <- sleep[sleep$p_id==p_id_select,]
  sleep_temp <- sleep_temp[sleep_temp$date_str >= date_select - days(win_size) & sleep_temp$date_str < date_select,]
  sleep_temp <- na.omit(sleep_temp)
  qids$sleep_day[i] <- nrow(sleep_temp)
  if (nrow(sleep_temp) >= 2){
    for (j in 1:length(sleep_feature)){
      # all days
      qids[i, (j-1) *length(second_feature) + 1 + pre_col_num + 1] = mean(sleep_temp[,j+5], na.rm = TRUE)
      qids[i, (j-1) *length(second_feature) + 2 + pre_col_num + 1] = sd(sleep_temp[,j+5], na.rm = TRUE)
    }
  }
}
# BT features ----

bt_feature <- colnames(bt)[7:11]
new_feature =c()

for (i in 1:length(bt_feature)){
  for (j in 1:length(second_feature)){
    new_feature = append(new_feature,paste(bt_feature[i], second_feature[j],sep = ""))
  }
}

pre_col_num <- ncol(qids)
qids$bt_day <- NA
qids[, new_feature] <-NA

for (i in 1:nrow(qids)){
  print(i)
  p_id_select <- qids$p_id[i]
  date_select <- qids$survey_date[i]
  bt_temp <- bt[bt$p_id==p_id_select,]
  bt_temp <- bt_temp[bt_temp$date_str >= date_select - days(win_size) & bt_temp$date_str < date_select,]
  bt_temp <- bt_temp[!is.na(bt_temp$bt_available_day), ]
  qids$bt_day[i] <- sum(bt_temp$bt_available_day >=12)   #change here for missing data threshold
  bt_temp <- bt_temp[bt_temp$bt_available_day >=12,]     #change here for missing data threshold
  
  if (nrow(bt_temp)>=2){
    for (j in 1:length(bt_feature)){
      # all day
      qids[i, (j-1) *length(second_feature) + 1 + pre_col_num + 1] = mean(bt_temp[,j+6], na.rm = TRUE)
      qids[i, (j-1) *length(second_feature) + 2 + pre_col_num + 1] = sd(bt_temp[,j+6], na.rm = TRUE)
      
    }
    
  }
}

# GPS ----

gps_feature <- colnames(gps)[7:9]
new_feature =c()
for (i in 1:length(gps_feature)){
  for (j in 1:length(second_feature)){
    new_feature = append(new_feature,paste(gps_feature[i], second_feature[j],sep = ""))
  }
}
pre_col_num <- ncol(qids)
qids$gps_day <- NA
qids[, new_feature] <-NA

for (i in 1:nrow(qids)){
  print(i)
  p_id_select <- qids$p_id[i]
  date_select <- qids$survey_date[i]
  gps_temp <- gps[gps$p_id==p_id_select,]
  gps_temp <- gps_temp[gps_temp$date_str >= date_select - days(win_size) & gps_temp$date_str < date_select,]
  gps_temp <- gps_temp[!is.na(gps_temp$gps_available), ]
  
  qids$gps_day[i] <- sum(gps_temp$gps_available >=12)
  gps_temp <- gps_temp[gps_temp$gps_available >=12,]
  
  if (nrow(gps_temp)>=1){
    for (j in 1:length(gps_feature)){
      # all day
      qids[i, (j-1) *length(second_feature) + 1 + pre_col_num + 1] = mean(gps_temp[,j+6], na.rm = TRUE)
      qids[i, (j-1) *length(second_feature) + 2 + pre_col_num + 1] = sd(gps_temp[,j+6], na.rm = TRUE)
    }
  }
}


## now come back -----
phq_prmt<-qids



### ### 
##  REMEMBER TO EXCLUDE:     ####
# f5978923-cef2-4eeb-a49c-7d79be4b53eb
# 5786af5e-99e3-4f78-848a-d3b67b8eb7ed
### ###


###### V. important exclusion
phq_prmt <- phq_prmt %>% 
  filter(p_id != "f5978923-cef2-4eeb-a49c-7d79be4b53eb", p_id != "5786af5e-99e3-4f78-848a-d3b67b8eb7ed")


phq_temp <- phq_prmt %>%
  filter(sleep_day >=2) # if there are more than 2 days. filtering for at least 2 days of sleep per week



# PHQ for other FITBIT DATA #####

# Date ID
Date_ID_set <- read_excel("HR_210322.xlsx", sheet = "Time Interval ID") %>%
  rename(DATE_ID = timeInterval_ID) %>%
  rename(date_str = datetimeStart)
# read HR feature table
HR_set <- read_excel("HR_210322.xlsx", sheet = "HR") %>%
  dplyr::select(SUBJECT_ID, DATE_ID, MISSING_RATE, FEATURE_001, FEATURE_002, FEATURE_003, FEATURE_004, FEATURE_005
                , FEATURE_006, FEATURE_007, FEATURE_008, FEATURE_009, FEATURE_010, FEATURE_015, FEATURE_016, FEATURE_017
                , FEATURE_018, FEATURE_019, FEATURE_020)
# map dateid
HR_set <- merge(HR_set,Date_ID_set) %>%
  dplyr::select(-DATE_ID) %>%
  dplyr::mutate(date_str = lubridate::ymd(date_str))%>%
  rename(p_id=SUBJECT_ID)

# read Step feature table
Step_set <- read_excel("STEPS_210322.xlsx", sheet = "Hoja1") %>%
  dplyr::select(SUBJECT_ID, DATE_ID, MISSING_RATE, FEATURE_001, FEATURE_002, FEATURE_003, FEATURE_004, FEATURE_005
                , FEATURE_006, FEATURE_007, FEATURE_019, FEATURE_020, FEATURE_022
                , FEATURE_024, FEATURE_025, FEATURE_026)
# map date id
Step_set <- merge(Step_set,Date_ID_set) %>%
  dplyr::select(-DATE_ID) %>%
  dplyr::mutate(date_str = lubridate::ymd(date_str))%>%
  rename(p_id=SUBJECT_ID)

# read Activity feature table
Activity <- read_excel("ACTIVITY_210322.xlsx", sheet = "Hoja1") %>%
  dplyr::select(SUBJECT_ID, DATE_ID, MISSING_RATE,FEATURE_001,FEATURE_002,FEATURE_003,FEATURE_004,FEATURE_005
                ,FEATURE_006,FEATURE_007,FEATURE_008,FEATURE_009,FEATURE_010,FEATURE_011,FEATURE_012,FEATURE_013
                ,FEATURE_014,FEATURE_015,FEATURE_016,FEATURE_017,FEATURE_018)
# map date id
Activity <- merge(Activity, Date_ID_set) %>%
  dplyr::select(-DATE_ID) %>%
  dplyr::mutate(date_str = lubridate::ymd(date_str))%>%
  rename(p_id=SUBJECT_ID)



# extract second-order features for each record

###change from phq to qids to run code
qids<-phq


# HR
# extract second-order hr features
hr_feature <- colnames(HR_set)[2:18]
second_feature <- c("_mean_hr", "_std_hr")
new_feature =c()
for (i in 1:length(hr_feature)){
  for (j in 1:length(second_feature)){
    new_feature = append(new_feature,paste(hr_feature[i],second_feature[j],sep = ""))
  }
}
pre_col_num <- ncol(qids)
qids$hr_day <- NA
qids[, new_feature] <-NA

for (i in 1:nrow(qids)){
  print(i)
  p_id_select <- qids$p_id[i]
  date_select <- qids$survey_date[i]
  hr_temp <- HR_set[HR_set$p_id==p_id_select,]
  hr_temp <- hr_temp[hr_temp$date_str >= date_select - days(win_size) & hr_temp$date_str < date_select,]
  # threshold for daily hr missing rate
  hr_temp <- hr_temp[hr_temp$MISSING_RATE < 50,]
  
  qids$hr_day[i] <- nrow(hr_temp)
  if (nrow(hr_temp) >= 2){
    for (j in 1:length(hr_feature)){
      # all days
      qids[i, (j-1) *length(second_feature) + 1 + pre_col_num + 1] = mean(as.numeric(hr_temp[,j+1]), na.rm = TRUE)
      qids[i, (j-1) *length(second_feature) + 2 + pre_col_num + 1] = sd(as.numeric(hr_temp[,j+1]), na.rm = TRUE)
    }
  }
}

# step features
step_feature <- colnames(Step_set)[2:15]
second_feature <- c("_mean_step", "_std_step")
new_feature =c()
for (i in 1:length(step_feature)){
  for (j in 1:length(second_feature)){
    new_feature = append(new_feature,paste(step_feature[i],second_feature[j],sep = ""))
  }
}
pre_col_num <- ncol(qids)
qids$step_day <- NA
qids[, new_feature] <-NA

for (i in 1:nrow(qids)){
  print(i)
  p_id_select <- qids$p_id[i]
  date_select <- qids$survey_date[i]
  step_temp <- Step_set[Step_set$p_id==p_id_select,]
  step_temp <- step_temp[step_temp$date_str >= date_select - days(win_size) & step_temp$date_str < date_select,]
  # threshold for daily hr missing rate
  step_temp <- step_temp[step_temp$MISSING_RATE < 50,]
  
  qids$step_day[i] <- nrow(step_temp)
  if (nrow(step_temp) >= 2){
    for (j in 1:length(step_feature)){
      # all days
      qids[i, (j-1) *length(second_feature) + 1 + pre_col_num + 1] = mean(as.numeric(step_temp[,j+1]), na.rm = TRUE)
      qids[i, (j-1) *length(second_feature) + 2 + pre_col_num + 1] = sd(as.numeric(step_temp[,j+1]), na.rm = TRUE)
    }
  }
}


# activity features
activity_feature <- colnames(Activity)[2:20]
second_feature <- c("_mean_Activity", "_std_Activity")
new_feature =c()
for (i in 1:length(activity_feature)){
  for (j in 1:length(second_feature)){
    new_feature = append(new_feature,paste(activity_feature[i],second_feature[j],sep = ""))
  }
}
pre_col_num <- ncol(qids)
qids$activity_day <- NA
qids[, new_feature] <-NA

for (i in 1:nrow(qids)){
  print(i)
  p_id_select <- qids$p_id[i]
  date_select <- qids$survey_date[i]
  activity_temp <- Activity[Activity$p_id==p_id_select,]
  activity_temp <- activity_temp[activity_temp$date_str >= date_select - days(win_size) & activity_temp$date_str < date_select,]
  # threshold for daily hr missing rate
  activity_temp <- activity_temp[activity_temp$MISSING_RATE < 50,]
  
  qids$activity_day[i] <- nrow(activity_temp)
  if (nrow(activity_temp) >= 2){
    for (j in 1:length(activity_feature)){
      # all days
      qids[i, (j-1) *length(second_feature) + 1 + pre_col_num + 1] = mean(as.numeric(activity_temp[,j+1]), na.rm = TRUE)
      qids[i, (j-1) *length(second_feature) + 2 + pre_col_num + 1] = sd(as.numeric(activity_temp[,j+1]), na.rm = TRUE)
    }
  }
}



### change back ---
phq_fitbit <- qids


phq_temp <- phq_fitbit %>%
  filter(hr_day >=1)


###### V. important exclusion
phq_fitbit <- phq_fitbit %>% 
  filter(p_id != "f5978923-cef2-4eeb-a49c-7d79be4b53eb", p_id != "5786af5e-99e3-4f78-848a-d3b67b8eb7ed")

phq_temp <- phq_temp %>% 
  filter(p_id != "f5978923-cef2-4eeb-a49c-7d79be4b53eb", p_id != "5786af5e-99e3-4f78-848a-d3b67b8eb7ed")


# combine both second_features! -----
phq_passive <- merge(phq_prmt, phq_fitbit)


## exclude variables with MISSING DATA -----

# missing data <= 50%       =  impute
# missing data > 50%       =  remove variable

#show how many NAs per variable and make a list of them named "a"
a <- as.data.frame((sapply(phq_passive, function(x) sum(is.na(x))))/nrow(phq_passive))
a <- as.data.frame(which(a > .5, arr.ind = TRUE))
low_data <- a$row

names(phq_passive)  #show me the names of all variables

other_vars <- c(3, 19, 
                22:23,  #time in bed
                40, 51, 58:60, 
                65:70, 81:86, 89:90, #HR
                93:95, 
                98:103, 112:113, 116:121, #steps
                122:124,
                133:148, 155:158) #activity   

length(phq_passive)    # 160
length(other_vars)     # 63
length(low_data)       # 16
160-63-16              # 81



phq_avail <- phq_passive[,c(-c(low_data), -c(other_vars))]
inc_vars <- phq_avail[, -c(1:17)]  # for correlation plot



#  ### ### ### ###  ###
# LOAD DESCRIPTIVES.R ####
#  ### ### ### ###  ###


IDmap <- read_excel("4. aRMT data.xlsx", sheet = "IDmap")

###  Descriptives  ###

demographics <- read_excel("2. Demographic Table.xlsx", sheet = "Demographics (R)") %>%
  select(c(1:27)) %>%
  rename(p_id = P_ID) %>%
  rename(record_id = record_ID) %>%
  rename(date_of_assessment = `Date of assessment`) %>% 
  rename(tx_start = `Treatment start date`) %>%
  rename(tx_end = `Tx end`) %>%
  rename(tx_length = `Tx length (weeks)`) %>%
  rename(tx_start_days = `Tx start lag (days)`) %>%
  mutate(tx_start_days = as.numeric(tx_start_days)) %>%
  mutate(gender = as.factor(Gender) %>% 
           fct_recode("male" = "Male", "female" = "Female", "other" = "Neither")) %>% 
  mutate(ethnicity = as.factor(Ethnicity) %>% 
           fct_recode(
             "Asian / Asian British / Any other Asian background" = "Asian / Asian British: Indian", 
             "Asian / Asian British / Any other Asian background" = "Any other Asian background (Please describe)",
             "Black / African / Caribbean / Black British / Any other Black background" = "Black / African / Caribbean / Black British: Caribbean",
             "Black / African / Caribbean / Black British / Any other Black background" = "Any other Black / African / Caribbean background (Please describe)",
             "Black / African / Caribbean / Black British / Any other Black background" = "Black / African / Caribbean / Black British: African",
             "Middle Eastern" = "Middle Eastern",
             "Mixed/Multiple ethnic groups" = "Mixed/Multiple ethnic groups: White and Black Caribbean",
             "Mixed/Multiple ethnic groups" = "Any other Mixed / Multiple ethnic background (Please describe)",
             "White / White British / Any other White background" = "Any other White background (please describe)", 
             "White / White British / Any other White background" = "White: English / Welsh / Scottish / Northern Irish / British",
             "White / White British / Any other White background" = "White: Irish",
           )) %>% 
  mutate(education_level = as.factor(`What is your highest completed level of education?`) %>% 
           fct_recode(
             "Degree level" = "Degree level education / diploma (e.g. BSc, BA)",
             "Post-graduate Degree" = "Post-graduate Degree (e.g. MSc, MA, PhD)",                                                        
             "College level or equivalent" = "College level education or equivalent (A lever, NVQ, International Baccaleureate, BTEC nationals)",
             "Secondary education" = "Secondary education (GCSE, O Levels)"   
           )) %>% 
  mutate(employment = as.factor(`What is your main employment status?`)) %>% 
  select(-c(Gender, DoB, Ethnicity, `What is your main employment status?`, `What is your highest completed level of education?` ))
demographics$tx_start = convertToDate(demographics$tx_start, origin = "1900-01-01")
demographics$tx_end = convertToDate(demographics$tx_end, origin = "1900-01-01")

names(demographics)
names(demographics[c(1,4:7, 14,23,24,25,26)])      # demographic variables from script = descriptives.R 

# create variables:
## baseline data + mid-point data + endpoint data ----
d <-merge(phq_avail, demographics[c(1,4:7, 14,23,24,25,26)])

phq_tx <- d %>% 
  #create baseline tx variable
  mutate(baselinediff = as.double(difftime(survey_date, tx_start, units = "days"))) %>%
  drop_na(tx_start) %>%
  mutate(baseline = ifelse(baselinediff > -15 & baselinediff < 8, 1, 0)) %>%
  
  #create endpoint phq
  mutate(enddiff = as.double(difftime(survey_date, tx_end, units = "days"))) %>%
  drop_na(tx_end) %>%
  mutate(end = ifelse(tx_length > 2 & enddiff > -8 & enddiff < 15, 1, 0)) %>%
  
  #create midpoint for tx
  mutate(half_tx = (as.numeric(tx_length)/2)*7) %>%
  mutate(mid = ifelse(tx_length > 3 & baselinediff > (half_tx-8) & baselinediff < (half_tx + 8), 1, 0)) |>
  
  #create tx_time as factor
  mutate(tx_time = as.factor(case_when(
    baseline == 1 ~ "baseline",
    mid == 1 ~ "mid",
    end ==1 ~ "end"))) |>
  
  select(-c("baselinediff", "baseline", "enddiff", "end", "half_tx", "mid"))



# View(phq_tx[c("survey_date", "tx_start", "baseline", 
#               "tx_end", "end","baselinediff", "half_tx", "mid")])

# test <- replace_with_na_all(phq_tx, condition = ~.x == "NaN")


## treatment data with no digital features -----
d <-merge(phq, demographics[c(1,4:7, 14,23,24,25,26)])


phq_tx_nofeatures <- d %>% 
  #create baseline tx variable
  mutate(baselinediff = as.double(difftime(survey_date, tx_start, units = "days"))) %>%
  drop_na(tx_start) %>%
  mutate(baseline = ifelse(baselinediff > -15 & baselinediff < 8, 1, 0)) %>%
  
  #create endpoint phq
  mutate(enddiff = as.double(difftime(survey_date, tx_end, units = "days"))) %>%
  drop_na(tx_end) %>%
  mutate(end = ifelse(tx_length > 2 & enddiff > -8 & enddiff < 15, 1, 0)) %>%
  
  #create midpoint for tx
  mutate(half_tx = (as.numeric(tx_length)/2)*7) %>%
  mutate(mid = ifelse(tx_length > 3 & baselinediff > (half_tx-8) & baselinediff < (half_tx + 8), 1, 0)) |>
  
  #create tx_time as factor
  mutate(tx_time = as.factor(case_when(
    baseline == 1 ~ "baseline",
    mid == 1 ~ "mid",
    end ==1 ~ "end"))) |>   
  
  select(-c("baselinediff", "baseline", "enddiff", "end", "half_tx", "mid"))

#create phq in wide format
phq_wide <- phq_tx_nofeatures |>
  select(record_id, redcap_event, total_phq) |>
  spread(key = redcap_event, value = total_phq)
phq_wide_2 <- phq_tx_nofeatures |>
  select(record_id, redcap_event, tx_time) |>
  spread(key = redcap_event, value = tx_time)

phq_bind <- rbind(phq_wide, phq_wide_2)

phq_nof <- phq_tx_nofeatures |>
  #remove those without who did not get treatment
  filter(!record_id %in% c(30, 31, 37, 45, 47, 62, 63)) |> 
  #treatment < 3 weeks
  filter(!record_id %in% c(18, 34, 37, 53)) |>
  
  filter(!is.na(tx_time))

#group any multiple values of tx_time (e.g. 2 baselines)
phq_time <- phq_nof |>
  select(tx_time, total_phq, record_id, redcap_event) |>
  group_by(record_id, tx_time) |>
  summarise(total_phq = mean(total_phq),
            redcap_event = mean(redcap_event),.groups = "keep") |>
  spread(key = tx_time, value = total_phq) |>
  relocate(c("record_id", "baseline", "mid", "end" ))

no_id <- phq_time[,2:4]

##imputation ----

# imputed <- no_id |>
#   apply_imputation(FUN = median, type = "rowwise")
# 
# write.csv(phq_time,"C:/Users/k1754359/Downloads/phq_time.csv")
# missing phq 9 values were imputed by hand using IAPT data


# -----> setwd <---------
# setwd("C:/Users/k1754359/OneDrive - King's College London/PhD/7. Predicting Treatment Outcome")
phq_imputed <- read_excel("treatment data.xlsx", sheet = "phq_time_r",
                          col_types = "numeric")


phq_long <- phq_imputed |>
  pivot_longer(
    cols = "phq_baseline":"phq_end",
    names_to = "tx_time",
    values_to = "phq_total") |>
  select(-c("redcap_baseline":"redcap_end"))
phq_long$tx_time <- phq_long$tx_time %>%      
  str_remove_all("phq_")

redcap_long <- phq_imputed |>
  pivot_longer(
    cols = "redcap_baseline":"redcap_end",
    names_to = "tx_time",
    values_to = "redcap_event") |>
  select(-c("phq_baseline":"phq_end"))
redcap_long$tx_time <- redcap_long$tx_time %>%        
  str_remove_all("redcap_")
imp_merged <- merge(redcap_long, phq_long) 


#join imputed phqs with features
#first join with existing weeks
complete <- merge(imp_merged, phq_tx, by = c("record_id", "redcap_event"), all.y = FALSE )

#get a df with missing data and complete with existing phq df scores, i.e. phq_avail
incomplete <- merge(imp_merged, phq_tx, by = c("record_id", "redcap_event"),  all.x = TRUE, all.y = FALSE ) |>
  filter(is.na(p_id)) |>
  select(c("record_id","tx_time.x","redcap_event","phq_total"))

incomplete$redcap_event_plus <- incomplete$redcap_event +1
incomplete$redcap_event_minus <- incomplete$redcap_event -1


a <- incomplete |>
  select(-redcap_event) |>
  rename(redcap_event = redcap_event_plus) |>
  select(-redcap_event_minus)

b <- incomplete |>
  select(-redcap_event) |>
  rename(redcap_event = redcap_event_minus) |>
  select(-redcap_event_plus) 

c <- rbind(a,b)

#add features to the missing data phq rows
regained <- c|>
  merge(phq_avail, all.x = TRUE, all.y = FALSE) |>
  drop_na(total_phq)

#we gain back some of the missing values from the original dataset
new_phqs <- regained |>
  select(-c("survey_date", "p_id")) |>
  group_by(record_id, tx_time.x) |>
  summarise(across(where(is.numeric), ~ mean(.x, na.rm = TRUE)), .groups = "keep")

#and we then merge it with the complete df
phq_tx_time <- merge(new_phqs, complete, all = TRUE) |>
  select(-c("p_id","Age", "gender", "ethnicity", "tx_start", "tx_start_days","tx_end","tx_length","education_level","employment","phq_binary"))

a <- read_excel("2. Demographic Table.xlsx", sheet = "Demographics (R)") %>%
  select(c(2,15,17,66)) %>%
  rename(record_id = record_ID) |>
  rename(age = `Age Rounddown`) |>
  rename(gender = Gender) |>
  rename(tx_intensity = `Therapy Intensity`)

phq_tx_time <- phq_tx_time |>
  merge(a, by = "record_id") |>
  mutate(tx_intensity = as.factor(tx_intensity)) |>
  select(-c("tx_time.y", "total_phq", )) |>
  rename(tx_time = tx_time.x) |>
  mutate(phq_binary = ifelse(phq_total >9, 1, 0))
phq_tx_time$tx_time <- factor(phq_tx_time$tx_time, levels=c("baseline", "mid", "end"))

##### complete df #####
View(phq_tx_time)




#create "change" dataframes ====

# names(phq_tx_time)


#create 3 separate dfs - one for each time-point
base <- phq_tx_time |> filter(tx_time == "baseline") |>  # n = 52
  select(where(is.numeric))
mid <- phq_tx_time |> filter(tx_time == "mid") |>        # n = 44
  select(where(is.numeric))
end <- phq_tx_time |> filter(tx_time == "end") |>        # n = 39
  select(where(is.numeric))

#these are the IDs that made it to the end
include <- end$record_id
include_mid <- mid$record_id

#exclude record_ids not in the end sample
base_exc <- base |> filter(record_id %in% include)
base_mid_exc <- mid |> filter(record_id %in% include)

base4mid <- base_exc |> filter(record_id %in% include_mid)
end4mid <- end |> filter(record_id %in% include_mid)


#change in features from baseline to endpoint
change <- end-base_exc
change_mid <- base_mid_exc-base4mid


# The IAPT definition of recovery is:
# 1. case at baseline
# 2. non-case at follow- up

#case at baseline
case <- base_exc |> mutate(case = ifelse(phq_total > 9, 1, 0))
#non-case at end
non_case <- end |> mutate(non_case = ifelse(phq_total < 10, 1, 0))

#RECOVERY, therefore:
recovery  <-  as.factor(ifelse(case$case + non_case$non_case == 2, 1, 0))
sum(recovery =="1")

rec <- cbind(base_exc, recovery)        
change_rec <- cbind(change, recovery)


#RELIABLE IMPROVEMENT <5 points change
rec$re_imp <- as.factor(ifelse(((end$phq_total)+5) < (base_exc$phq_total), 1, 0))
rec$re_rec <- as.factor(ifelse(rec$recovery == 1 & rec$re_imp == 1, 1, 0))

change_rec$re_imp <- as.factor(ifelse(((end$phq_total)+5) < (base_exc$phq_total), 1, 0))
change_rec$re_rec <- as.factor(ifelse(change_rec$recovery == 1 & change_rec$re_imp == 1, 1, 0))

sum(change_rec$recovery =="1")

worse <- ((end$phq_total)-5) > (base_exc$phq_total) # 1

#reorder
rec <- rec |> select(record_id, redcap_event, phq_total, recovery, re_imp, re_rec, phq_binary, age, everything())
change_rec <- change_rec |> select(record_id, redcap_event, phq_total, recovery, re_imp, re_rec, everything())



#now do the same but for mid (who is missing p 67)
case <- base4mid |> mutate(case = ifelse(phq_total > 9, 1, 0))
non_case <- end |> filter(record_id %in% include_mid) |>   mutate(non_case = ifelse(phq_total < 10, 1, 0))
recovery  <-  as.factor(ifelse(case$case + non_case$non_case == 2, 1, 0))
mid_rec <- cbind(change_mid, recovery)

mid_rec$re_imp <- as.factor(ifelse(((end4mid$phq_total)+5) < (base4mid$phq_total), 1, 0))
mid_rec$re_rec <- as.factor(ifelse(mid_rec$recovery == 1 & mid_rec$re_imp == 1, 1, 0))
#reorder
mid_rec <- mid_rec |> select(record_id, redcap_event, phq_total, recovery, re_imp, re_rec, everything())


tibble(rec)         # outcome 1 ----
tibble(change_rec)  # outcome 2 ----
tibble(mid_rec)     # outcome 3 ----


#compare those with no end date
ret<-phq_tx_time |>
  mutate(retained = ifelse(record_id %in% include, 1, 0))

#are baseline PHQs different between those retained and those not?
# answer =  no, neither is age or gender * trend
x<- ret|> filter(retained == 1) |> select(phq_total)
y<- ret|> filter(retained == 0) |> select(phq_total)
t.test(x,y)

 
# re <- ret |> 
#   filter(gender != "Neither")
table(ret$gender, ret$retained)
fisher.test(ret$gender,ret$retained)

boxplot(change[,c(3, 15:78)])
boxplot(change[,c(3, 15:50)])



# Create df with baseline to mid point scores -----
# View(phq_tx)

# from phq_imputed take lowest baseline measures to create min_baseline variable
# i.e. when tx_time = baseline, what is the min(survey_date)
min_baseline <- phq_tx |>
  filter(tx_time == "baseline")|>
  group_by(record_id) |>
  select(record_id, redcap_event) |>
  summarise(min_baseline = min(redcap_event))


# from phq_imputed take highest midpoint measures to create midpoint variable
mipoint <- phq_imputed |>
  select(record_id, redcap_mid)

#now add both variables to df
phq_txa <- merge(phq_tx, min_baseline, by = "record_id", all.x = TRUE)
phq_txa <- merge(phq_txa, mipoint, by = "record_id", all.x = TRUE)

# phq_txa |>
#   select(record_id, redcap_event, survey_date, tx_time, tx_start, min_baseline, redcap_mid)


# calculate if redcap event < min baseline = 1
# calculate if redcap_event is > midpoint = 1
mid_tx_df <- phq_txa |>
  mutate(pre_base = ifelse(redcap_event < min_baseline, 1, 0)) |>
  mutate(post_mid = ifelse(redcap_event > redcap_mid, 1, 0)) |>
  filter(pre_base != 1) |>        # delete the 1s
  filter(post_mid != 1)

length(unique(mid_tx_df$record_id))   # 52

# now add recovery and re_imp variables
recovered <- rec |> select(record_id, recovery)
improved <- rec |> select(record_id, re_imp)

mid_tx_dfa <- merge(mid_tx_df, recovered)
mid_tx_dfa <- merge(mid_tx_dfa, improved) |>   # outcome 4 -----
  select(-c(82:95))

# REGRESSION ----

#impute missing data----
rec <- rec |>
  apply_imputation(FUN = median, type = "columnwise") |>
  relocate(c(age, phq_binary), .after = last_col())
change_rec <- change_rec |>
  apply_imputation(FUN = median, type = "columnwise")
mid_rec <- mid_rec |>
  apply_imputation(FUN = median, type = "columnwise")
mid_tx_dfa <- mid_tx_dfa |>
  apply_imputation(FUN = median, type = "columnwise")



####  Loop Logistic Regression ------

names(rec)             # baseline
names(change_rec)      # change base to end
names(mid_rec)         # change base to mid
names(mid_tx_dfa)      # all weeks from baseline to mid


rec_features <- rec[,18:81]
change_rec_features <- change_rec[,18:81]
mid_rec_features <- mid_rec[,18:81]
mid_tx_dfa_features <- mid_tx_dfa[,18:81]

scaled_rec <- as.data.frame(scale(rec_features))
scaled_change_rec <- as.data.frame(scale(change_rec_features))
scaled_mid_rec <- as.data.frame(scale(mid_rec_features))
scaled_mid_tx <- as.data.frame(scale(mid_tx_dfa_features))

scaled_rec$recovery <- rec$recovery 
scaled_change_rec$recovery <- change_rec$recovery
scaled_mid_rec$recovery <- mid_rec$recovery 
scaled_mid_tx$recovery <- mid_tx_dfa$recovery 

scaled_rec$re_imp <- rec$re_imp 
scaled_change_rec$re_imp <- change_rec$re_imp 
scaled_mid_rec$re_imp <- mid_rec$re_imp 
scaled_mid_tx$re_imp <- mid_tx_dfa$re_imp 

# The outcome needs to be defined as a factor
scaled_rec$recovery<-as.factor(scaled_rec$recovery) 
levels(scaled_rec$recovery) <- c("not", "recovered")
scaled_change_rec$recovery<-as.factor(scaled_change_rec$recovery) 
levels(scaled_change_rec$recovery) <- c("not", "recovered")
scaled_mid_rec$recovery<-as.factor(scaled_mid_rec$recovery) 
levels(scaled_mid_rec$recovery) <- c("not", "recovered")
scaled_mid_tx$recovery<-as.factor(scaled_mid_tx$recovery) 
levels(scaled_mid_tx$recovery) <- c("not", "recovered")

#re_imp factor
scaled_rec$re_imp<-as.factor(scaled_rec$re_imp) 
levels(scaled_rec$re_imp) <- c("not", "improved")
scaled_change_rec$re_imp<-as.factor(scaled_change_rec$re_imp) 
levels(scaled_change_rec$re_imp) <- c("not", "improved")
scaled_mid_rec$re_imp<-as.factor(scaled_mid_rec$re_imp) 
levels(scaled_mid_rec$re_imp) <- c("not", "improved")
scaled_mid_tx$re_imp<-as.factor(scaled_mid_tx$re_imp) 
levels(scaled_mid_tx$re_imp) <- c("not", "improved")


data = scaled_mid_rec   # <---------- CHANGE here depending on df

FEATURE <- c()
Estimate <- c()
StdError <- c()
z_value <- c()
pr_t <- c()
OR <-c()
lowerCI<-c()
upperCI<-c()
## Basic
for (i in 1:62) {    # <---------- CHANGE here depending on feature
  # pb$tick()
  fit <-glm(recovery ~ data[[i]], data = data, family = "binomial")  # no covariates included
  
  FEATURE <- append(FEATURE,toupper(names(data)[i]))
  Estimate <- append(Estimate,round(coef(summary(fit)), digits = 6)[2,1])
  StdError <- append(StdError,round(coef(summary(fit)), digits = 6)[2,2])
  z_value <- append(z_value,round(coef(summary(fit)), digits = 6)[2,3])
  pr_t <- append(pr_t,round(coef(summary(fit)), digits = 6)[2,4])
  OR <- append(OR,round(exp(coef(fit)), digits = 6)[2])
  lowerCI <- append(lowerCI,round(exp(confint(fit)), digits = 6)[2,1])
  upperCI <- append(upperCI,round(exp(confint(fit)), digits = 6)[2,2])
  
  Sys.sleep(0.1)
  
}

results= data.frame(FEATURE,Estimate,StdError,z_value,pr_t, OR, lowerCI, upperCI)
colnames(results) <- c("FEATURE","Estimate","Std. eror","zvalue","pvalue", "OR", "lower CI", "upper CI")
# write.csv(results,"C:/Users/k1754359/OneDrive - King's College London/PhD/7. Predicting Treatment Outcome/results.csv",row.names = FALSE)
# write.csv(results,"C:/Users/k1754359/Downloads/results.csv",row.names = FALSE)
write.csv(results,"results.csv",row.names = FALSE)

results |> filter(pvalue < 0.05)


# =IF(E2<0.05, "*", "")


fit <-glm(recovery ~ REM_pct_mean  , data = data, family = "binomial")
summary(fit)


## Mixed effects logistic regression ----

#add record_id to scaled_mid_tx
scaled_mid_tx$record_id <- mid_tx_dfa$record_id

names(data)
data = mid_tx_dfa        #     <- - - - select the data to be used for the models

FEATURE <- c()
Estimate <- c()
StdError <- c()
z_value <- c()
pr_t <- c()
OR <-c()
lowerCI<-c()
upperCI<-c()


## Basic
for (i in 18:81) {    # <---------- CHANGE here depending on feature
  fit <-glm(re_imp
             ~ data[[i]]
             # + Age + gender
             + (1 |record_id),
            data = data,family = "binomial")
  
  FEATURE <- append(FEATURE,toupper(names(data)[i]))
  Estimate <- append(Estimate,round(coef(summary(fit)), digits = 6)[2,1])
  StdError <- append(StdError,round(coef(summary(fit)), digits = 6)[2,2])
  z_value <- append(z_value,round(coef(summary(fit)), digits = 6)[2,3])
  pr_t <- append(pr_t,round(coef(summary(fit)), digits = 6)[2,4])
  OR <- append(OR,round(exp(coef(fit)), digits = 6)[2])
  lowerCI <- append(lowerCI,round(exp(confint(fit)), digits = 6)[2,1])
  upperCI <- append(upperCI,round(exp(confint(fit)), digits = 6)[2,2])
  
  Sys.sleep(0.1)
  
}
results= data.frame(FEATURE,Estimate,StdError,z_value,pr_t, OR, lowerCI, upperCI)
colnames(results) <- c("FEATURE","Estimate","Std. eror","zvalue","pvalue", "OR", "lower CI", "upper CI")
# write.csv(results,"C:/Users/k1754359/OneDrive - King's College London/PhD/7. Predicting Treatment Outcome/results.csv",row.names = FALSE)
# write.csv(results,"C:/Users/k1754359/Downloads/results.csv",row.names = FALSE)
write.csv(results,"results.csv",row.names = FALSE)

results |> filter(pvalue < 0.05)


# =IF(E2<0.05, "*", "")



# Plot the data ----

#extract activity as one of the features e.g. FEATURE_006_step_mean and recovery as outcome df
# saves dataframe with two columns: activity change & recovery
dat=as.data.frame(cbind(activity, recov))
dat$recov <- factor(dat$recov, labels = c(0,1))
dat
# Plot with activity on x-axis and recovery (0 or 1) on y-axis
plot(activity,recov,xlab="activity",ylab="Recovery") 

# Logistic regression model (in this case, generalized linear model 
# with logit link). see ?glm
mylogit=glm(recov~activity,family=binomial,data = dat) 
summary(mylogit)

# CIs using standard errors
confint.default(mylogit)
# Odds ratios and 95% CI
exp(coef(fit))
exp(confint(mylogit))
# Nicer output
exp(cbind(OR = coef(mylogit), confint(mylogit)))


# Plot the predicted probability of recovery
# Plot with activity on x-axis and recovery (0 or 1) on y-axis
plot(activity,recov,xlab="activity",ylab="Recovery") 
# Add a prediction curve based on your logistic regression model to your plot
# "curve" draws a curve based on prediction from logistic regression model
curve(predict(mylogit,data.frame(activity=x),type="response"),add=TRUE)


# optional: this draws an the predicted probabilities for each case 
# based on a fit "mylogit" to glm model. pch= changes type of dots.
points(activity,fitted(mylogit),pch=20)



##### Regularized logistic regression #### 

#predicting recovery from:
# 1. baseline behaviour scores

names(rec)
names(change_rec)


x<-as.matrix(change_rec[,20:83])
x <- scale(x)
y<-as.factor(change_rec[,"re_imp"])  # <- change from recovery to re_imp (reliable improvement)

# Regularized logistic regression (familiy = binomial calls logistic regression)
# Note alpha = 1 for lasso only (penalty down to alpha=0 ridge only)
# We fit the model and store the results in the object "glmmod"
# glmmod contains all information of the fitted model 
glmmod<-glmnet(x,y,alpha=1,family='binomial')


# Visualize the coefficients by executing the plot function:
# Each curve corresponds to a variable: 
# The curve shows the path of its coefficient against log of lambda of the whole coefficient vector at as ???? varies.
# The axis on the top shows the number of nonzero coefficients at the current ??
# The y axis shows the regression coefficents of a variable at current ??  
plot(glmmod, "lambda" )
# If we use the print function we will get a summary of glmnet at each step of ??
# including the number of included varibales (Df) and the % explained DEviance (%Dev) 
print(glmmod) 


# Select best lambda using 10-fold cross-validation based on sum of deviance residuals 
# If we want  missclassification error as selection criteria we need to include <<type.measure = "class">>
set.seed(101) # remove if you want to get a different random sample of CV
# set.seed(123)
# set.seed(42)
cv.glmmod<- cv.glmnet(x,y,family='binomial')
# Plot the object. It shows the cross-validation curve (and upper and lower standard devitaion) along the ?? sequence 
# On the y-axis is a measure of the model goodness (here: Binomial deviance: the samller the deviance the better the model)
# The two horizontal lines shows the lambda of minimum deviance and the lambda 1 SE larger than the minimal lambda (more regularized than minimum lambda)
plot(cv.glmmod, xvar="lambda")
# To view the two lambdas type:
cv.glmmod$lambda.min
cv.glmmod$lambda.1se
# To see the regression coefficents at minimum or minimum + 1se lambda
coef(cv.glmmod, s = "lambda.min")
coef(cv.glmmod, s = "lambda.1se")
# Save the minimum and 1se below minimum lambda 
best_lambda <- cv.glmmod$lambda.min
best_lambda1se <- cv.glmmod$lambda.1se 


# We now predcit the probability for a case to belong to the recovery group 
# using minimum lambda
y_prob<-predict(cv.glmmod,type="response",newx=x, s = "lambda.min")
# Or we can predict to which class a case belongs (recovery or non-recovered) 
y_pred<-as.numeric(predict(cv.glmmod,type="class",newx=x, s = "lambda.min"))  # comment roc command needs predction to be numberic
# Plot a cross tbale of observed and predicted
table(y,y_pred)
# Get a summary of prediction accuracy (% correct) and measure of agreement between 
# observed and predictied cases (kappa)
print(postResample(pred=as.factor(y_pred), obs=y))
# Get sensitivity (true negatives) and speccifity (true positives)  
sensitivity(as.factor(y_pred), y)
specificity(as.factor(y_pred), y)
# Get more information about your model prediction quality 
# positive defines the treatment (or positive) group, here high risk is coded as 1 
# and will be used as "positive" group 
confusionMatrix(as.factor(y_pred), y, positive="1")
# For more information, see help 


# AUC is a measure of discrimination (eqivalent to the Concordance or C statistics)
# One way of interpreting AUC is as the probability that the model ranks a random positive example more highly than a random negative example.
roc_obj.train <- roc(y, (y_pred))
roc_obj.train

# Important : These measures of prediction accuracy are overoptimisitc because they are 
# they are estimated from the same data set we used for model selection. 
# These validity measures are called apparent validity!  
# To obtain internal validity, we need to hold out a sample before we select (using CV) and fit the model 
 



### # Repeated cross-validation ###### 

#  Optional: Repeating n-fold crossvalidatoin usig loop ##
#  Selecting lambda based on a single run of 10-fold cross-validation is usuallIy
#  not recommended. The procedure should be repeated 100 times (with different folds)
#  and the mean of each 100 minimum lanbdas (or 100 minimum +1 SE lambdas) should be used 
#  This can be asily done with a loop or we use the caret package and its function "trainControl" and train #
#  The package allows allows to use parallel computing: Large portions of code can run concurrently in different cores 
# and reduces the total time for the computation. To use paralelle computong we need to load the package "doParalell" 

library(doParallel)

names(change_rec)


rec_features <- change_rec[,20:83]
scaled_features <- as.data.frame(scale(rec_features))
scaled_features$recovery <- change_rec$re_imp  # <- ---- change here rec$recovery  / rec$re_imp  /  change_rec$recovery


# We need to use the combined data set with x and y
# The outcome needs to be defined as a factor

scaled_features$recovery<-as.factor(change_rec$re_imp) # <- ---- change here rec$recovery  / rec$re_imp
levels(scaled_features$recovery) <- c("not", "recovered")  # levels 0 and 1 aren't valid names in R and you need to label your two levels
y<-scaled_features$recovery
# Set up number of cores 
cl=makeCluster(4);registerDoParallel(cl)

# Set up training settings object
set.seed(123)
trControl <- trainControl(method = "repeatedcv", # repeated CV 
                          repeats = 10,          # number of repeated CV
                          number = 6   ,         # Number of folds
                          summaryFunction = twoClassSummary,  #function to compute performance metrics across resamples.AUC for binary outcomes
                          classProbs = TRUE, 
                          savePredictions = "all",
                          allowParallel = TRUE,
                          selectionFunction = "best" ) # best - minimum lambda, oneSE for minimum lambda + 1 Se, Tolerance for minimum + 3%


# Set up grid of parameters to test
params = expand.grid(alpha=c(1),   # L1 & L2 mixing parameter
                     lambda=2^seq(1,-10, by=-0.1)) # regularization parameter



# Run training over tuneGrid and select best model
glmnet.obj <- train(recovery ~ .,             # model formula (. means all features)
                    data = scaled_features,         # data.frame containing training set
                    method = "glmnet",     # model to use
                    metric ="ROC",         # Optimizes AUC, best with deviance for unbalanced outcomes 
                    family="binomial",     # logistic regression
                    trControl = trControl, # set training settings
                    tuneGrid = params)     # set grid of params to test over, if not specified defualt gris is used (not always the best)


stopCluster(cl) # Stop the use of cores!

# Plot performance for different params
plot(glmnet.obj, xTrans=log, xlab="log(lambda)")

# Plot regularization paths for the best model
plot(glmnet.obj$finalModel, xvar="lambda", label=T)

# Summary of main results 
# print(glmnet.obj)
#Variable importance of unstandardised coefficents
plot(varImp(glmnet.obj, scale = FALSE), top = 10, main = "glmnet- unstandardised coefficents")

# See the content of the object 
summary(glmnet.obj)
# str(glmnet.obj)

# glmnet.obj$results


get_best_result = function(caret_fit) {
  best = which(rownames(caret_fit$results) == rownames(caret_fit$bestTune))
  best_result = caret_fit$results[best, ]
  rownames(best_result) = NULL
  best_result
}

get_best_result(glmnet.obj)

best_alpha <-get_best_result(glmnet.obj)$alpha
best_lambda <- get_best_result(glmnet.obj)$lambda

# Model coefficients
# The fitted coefficients at the optimal penalties can be obtained by  using the  coef command 
coef(glmnet.obj$finalModel, glmnet.obj$bestTune$lambda)  # best model 

# Predcit values for the training data set (apparent validity)

predictions_prob<-predict(glmnet.obj,as.matrix(scaled_features[,-65],best_lambda), type="prob") #-65 is the index of the outcome variable
predictions_class<-predict(glmnet.obj,as.matrix(scaled_features[,-65],best_lambda), type="raw")

# Model prediction performance using caret functions (not Metrics)
# Get more information about your model prediction quality 
# positive defines the treatment (or positive) group, here high risk is coded as 1 
# and will be used as "positive" group 
confusionMatrix(as.factor(predictions_class), scaled_features[,65], positive="recovered")


# AUC is a measure of discrimination (eqivalent to the Concordance or C statistics)
# One way of interpreting AUC is as the probability that the model ranks a random positive example more highly than a random negative example.
roc_obj.train <- roc(scaled_features[,65], as.numeric(predictions_class), directions=">")
roc_obj.train
plot(roc_obj.train)

### Calibration alpha and beta, see lecture calibration
predictions_prob[predictions_prob==1]  <- 0.999999999
predictions_prob[predictions_prob==0]  <- 0.000000001
logOdds<-log(predictions_prob/(1- predictions_prob))
glm.coef       <-  glm(scaled_features$recovery ~ logOdds[,2],family=binomial)$coef  # choose column with proibability to be a case = 1
Alpha  <-  glm.coef[1]
Beta	 <-  glm.coef[2]
paste("Calibration slope beta is ", round(Beta,3))
paste("Calibration in the large alpha is ", round(Alpha,3))


#

